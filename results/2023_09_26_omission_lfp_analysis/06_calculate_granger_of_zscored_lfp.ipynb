{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "c3a02adc9e884466bc8c79db549cc3d2",
    "deepnote_cell_type": "text-cell-h1",
    "formattedRanges": [
     {
      "fromCodePoint": 0,
      "marks": {
       "bold": true,
       "underline": true
      },
      "toCodePoint": 17,
      "type": "marks"
     }
    ]
   },
   "source": [
    "# Omission LFP Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "4546bee655b14a5dbf393161f1228e60",
    "deepnote_cell_type": "text-cell-p",
    "formattedRanges": []
   },
   "source": [
    "Brief 1-2 sentence description of notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from collections import defaultdict\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"SPECTRAL_CONNECTIVITY_ENABLE_GPU\"] = \"true\"\n",
    "import cupy as cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cell_id": "03b495cefa6a4798a44c7f2e4c6a3ea7",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 21,
    "execution_start": 1691424003626,
    "source_hash": null
   },
   "outputs": [],
   "source": [
    "# Imports of all used packages and libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import itertools\n",
    "from scipy.stats import linregress\n",
    "import cupy as cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spikeinterface.extractors as se\n",
    "import spikeinterface.preprocessing as sp\n",
    "from spectral_connectivity import Multitaper, Connectivity\n",
    "import spectral_connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import grangercausalitytests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "d290bac2c17940bfbc0f9296beaf70e5",
    "deepnote_cell_type": "text-cell-h2",
    "formattedRanges": []
   },
   "source": [
    "## Inputs & Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "e528ce19c608425292151930d380f49f",
    "deepnote_cell_type": "text-cell-p",
    "formattedRanges": []
   },
   "source": [
    "Explanation of each input and where it comes from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_HALFBANDWIDTH_PRODUCT = 2\n",
    "TIME_WINDOW_DURATION = 1\n",
    "TIME_WINDOW_STEP = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "FREQ_MAX = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your keywords\n",
    "KEYWORD_TOP = '_trial_'\n",
    "KEYWORD_BOTTOM = '_baseline_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "GROUPING = \"all\"\n",
    "# GROUPING = \"velocity_parsed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs and Required data loading\n",
    "# input varaible names are in all caps snake case\n",
    "# Whenever an input changes or is used for processing \n",
    "# the vairables are all lower in snake case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BBOX_TO_ANCHOR=(1.5, 0.9)\n",
    "LOC='upper right'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_BANDS = [\"theta\", \"beta\", \"gamma\"]\n",
    "BAND_TO_FREQ = {\"theta\": {\"low_freq\": 4, \"high_freq\": 12}, \"beta\": {\"low_freq\": 13, \"high_freq\": 30}, \"gamma\": {\"low_freq\": 30, \"high_freq\": 70}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables for LFP extraction\n",
    "FREQ_MIN=0.5\n",
    "FREQ_MAX=300\n",
    "NOTCH_FREQ=60\n",
    "ORIGINAL_SAMPLE_RATE = 20000\n",
    "RESAMPLE_RATE=1000\n",
    "TRIAL_DURATION=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASELINE_OUTCOME_TO_COLOR = {\n",
    "'lose': \"red\",\n",
    "'lose_trial': \"red\",\n",
    "'lose_baseline': \"hotpink\",\n",
    "\n",
    "'omission': \"orange\",\n",
    "'omission_trial': \"orange\",\n",
    "'omission_baseline': \"navajowhite\",\n",
    "\n",
    "'rewarded': \"green\",\n",
    "'rewarded_trial': \"green\",\n",
    "'rewarded_baseline': \"lightgreen\",\n",
    "\n",
    "'win': \"blue\",\n",
    "'win_trial': \"blue\",\n",
    "'win_baseline': \"lightblue\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cell_id": "6cf83a5811054461a718a71673d09aab",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 373,
    "execution_start": 1691424003628,
    "source_hash": null
   },
   "outputs": [],
   "source": [
    "CHANNEL_MAPPING_DF = pd.read_excel(\"../../data/channel_mapping.xlsx\")\n",
    "CHANNEL_MAPPING_DF[\"Subject\"] = CHANNEL_MAPPING_DF[\"Subject\"].astype(str)\n",
    "\n",
    "TONE_TIMESTAMP_DF = pd.read_excel(\"../../data/rce_tone_timestamp.xlsx\", index_col=0)\n",
    "OUTPUT_DIR = r\"./proc\" # where data is saved should always be shown in the inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_col_name(col_name, keywords, replacement=\"_\"):\n",
    "    \"\"\"\n",
    "    Standardize the column name by removing specified keywords.\n",
    "\n",
    "    Parameters:\n",
    "    col_name (str): The original column name.\n",
    "    keywords (list of str): A list of keywords to remove from the column name.\n",
    "\n",
    "    Returns:\n",
    "    str: The standardized column name.\n",
    "    \"\"\"\n",
    "    for keyword in keywords:\n",
    "        col_name = col_name.replace(keyword, replacement)\n",
    "    # Replace any double underscores possibly created and strip leading/trailing underscores\n",
    "    col_name = col_name.replace('__', '_').strip('_')\n",
    "    return col_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "e3ee4891d43a4ac287413afc552ca289",
    "deepnote_cell_type": "text-cell-h2",
    "formattedRanges": []
   },
   "source": [
    "## Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "9ccbf6cc70fd4d379fa29317f733771f",
    "deepnote_cell_type": "text-cell-p",
    "formattedRanges": []
   },
   "source": [
    "Describe each output that the notebook creates. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "fc8e8920a6944918a15fac575cdf6e78",
    "deepnote_cell_type": "text-cell-bullet",
    "formattedRanges": []
   },
   "source": [
    "- Is it a plot or is it data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "1e639d4776a84aa9ac8ded2e14fa57db",
    "deepnote_cell_type": "text-cell-bullet",
    "formattedRanges": []
   },
   "source": [
    "- How valuable is the output and why is it valuable or useful?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "- Ideally functions are defined here first and then data is processed using the functions\n",
    "    - function names are short and in snake case all lowercase\n",
    "    - a function name should be unique but does not have to describe the function\n",
    "    - doc strings describe functions not function names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pairs(lst):\n",
    "    pairs = []\n",
    "    n = len(lst)\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            pairs.append((lst[i], lst[j]))\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_dict():\n",
    "    return defaultdict(dict)\n",
    "\n",
    "triple_nested_dict = defaultdict(nested_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "8999d19b6b7d4d63bc90f0b0bd9ab085",
    "deepnote_cell_type": "text-cell-h2",
    "formattedRanges": []
   },
   "source": [
    "## Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASIC_LFP_TRACES_DF = pd.read_pickle(\"./proc/full_baseline_and_trial_lfp_traces.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASIC_LFP_TRACES_DF = BASIC_LFP_TRACES_DF.drop_duplicates([\"recording_file\", \"time\", \"current_subject\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['time', 'recording_dir', 'recording_file', 'time_stamp_index',\n",
       "       'video_file', 'video_frame', 'video_number', 'subject_info',\n",
       "       'competition_closeness', 'video_name', 'all_subjects',\n",
       "       'current_subject', 'trial_outcome', 'lfp_index',\n",
       "       'baseline_lfp_timestamp_range', 'trial_lfp_timestamp_range',\n",
       "       'baseline_ephys_timestamp_range', 'trial_ephys_timestamp_range',\n",
       "       'baseline_videoframe_range', 'trial_videoframe_range', 'trial_number',\n",
       "       'Cohort', 'spike_interface_mPFC', 'spike_interface_vHPC',\n",
       "       'spike_interface_BLA', 'spike_interface_LH', 'spike_interface_MD',\n",
       "       'mPFC_baseline_lfp_trace', 'mPFC_trial_lfp_trace',\n",
       "       'vHPC_baseline_lfp_trace', 'vHPC_trial_lfp_trace',\n",
       "       'BLA_baseline_lfp_trace', 'BLA_trial_lfp_trace',\n",
       "       'LH_baseline_lfp_trace', 'LH_trial_lfp_trace', 'MD_baseline_lfp_trace',\n",
       "       'MD_trial_lfp_trace'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BASIC_LFP_TRACES_DF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>recording_dir</th>\n",
       "      <th>recording_file</th>\n",
       "      <th>time_stamp_index</th>\n",
       "      <th>video_file</th>\n",
       "      <th>video_frame</th>\n",
       "      <th>video_number</th>\n",
       "      <th>subject_info</th>\n",
       "      <th>competition_closeness</th>\n",
       "      <th>video_name</th>\n",
       "      <th>...</th>\n",
       "      <th>mPFC_baseline_lfp_trace</th>\n",
       "      <th>mPFC_trial_lfp_trace</th>\n",
       "      <th>vHPC_baseline_lfp_trace</th>\n",
       "      <th>vHPC_trial_lfp_trace</th>\n",
       "      <th>BLA_baseline_lfp_trace</th>\n",
       "      <th>BLA_trial_lfp_trace</th>\n",
       "      <th>LH_baseline_lfp_trace</th>\n",
       "      <th>LH_trial_lfp_trace</th>\n",
       "      <th>MD_baseline_lfp_trace</th>\n",
       "      <th>MD_trial_lfp_trace</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6310663</td>\n",
       "      <td>20221202_134600_omission_and_competition_subje...</td>\n",
       "      <td>20221202_134600_omission_and_competition_subje...</td>\n",
       "      <td>1390826</td>\n",
       "      <td>20221202_134600_omission_and_competition_subje...</td>\n",
       "      <td>1734</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6_1_top_2_base_3</td>\n",
       "      <td>rewarded</td>\n",
       "      <td>20221202_134600_omission_and_competition_subje...</td>\n",
       "      <td>...</td>\n",
       "      <td>[1.8457601, 1.7363818, 1.6475118, 1.59738, 1.2...</td>\n",
       "      <td>[0.6927297, 0.96389693, 0.7884358, -0.04101689...</td>\n",
       "      <td>[-0.06969439, -0.09568214, -0.05315674, 0.1571...</td>\n",
       "      <td>[1.5864334, 1.5710771, 1.5970649, 1.2155175, 0...</td>\n",
       "      <td>[2.0367627, 2.1163385, 2.1618104, 2.2679114, 2...</td>\n",
       "      <td>[0.3164087, 0.36377528, 0.18757163, -0.5020857...</td>\n",
       "      <td>[3.1382985, 3.2319791, 3.2788196, 3.2881875, 3...</td>\n",
       "      <td>[0.8118982, 1.2209699, 0.87435186, -0.4028264,...</td>\n",
       "      <td>[1.3934726, 1.494771, 1.764077, 1.828315, 1.68...</td>\n",
       "      <td>[-0.9783956, -0.86721426, -0.7288553, -1.40582...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7910662</td>\n",
       "      <td>20221202_134600_omission_and_competition_subje...</td>\n",
       "      <td>20221202_134600_omission_and_competition_subje...</td>\n",
       "      <td>2990825</td>\n",
       "      <td>20221202_134600_omission_and_competition_subje...</td>\n",
       "      <td>3728</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6_1_top_2_base_3</td>\n",
       "      <td>rewarded</td>\n",
       "      <td>20221202_134600_omission_and_competition_subje...</td>\n",
       "      <td>...</td>\n",
       "      <td>[1.2191132, 1.1348007, 1.2054409, 1.0960625, 0...</td>\n",
       "      <td>[1.0732753, 0.7246318, 0.7633699, 0.3782669, -...</td>\n",
       "      <td>[0.31539667, 0.23152715, 0.29767776, 0.4217101...</td>\n",
       "      <td>[0.03543783, -0.27641505, -0.40044746, -0.6638...</td>\n",
       "      <td>[0.3107247, 0.14209972, -0.05873455, -0.331566...</td>\n",
       "      <td>[0.026525281, -0.04547191, 0.11936376, -0.4092...</td>\n",
       "      <td>[-1.180375, -1.2959143, -1.3771042, -1.458294,...</td>\n",
       "      <td>[0.9492963, 0.46840277, 0.6713773, 0.043717593...</td>\n",
       "      <td>[-0.14577106, -0.16059524, 0.027177656, 0.1680...</td>\n",
       "      <td>[1.6281886, 1.349, 1.4675934, 0.9487473, -0.21...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9710660</td>\n",
       "      <td>20221202_134600_omission_and_competition_subje...</td>\n",
       "      <td>20221202_134600_omission_and_competition_subje...</td>\n",
       "      <td>4790823</td>\n",
       "      <td>20221202_134600_omission_and_competition_subje...</td>\n",
       "      <td>5972</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6_1_top_2_base_3</td>\n",
       "      <td>rewarded</td>\n",
       "      <td>20221202_134600_omission_and_competition_subje...</td>\n",
       "      <td>...</td>\n",
       "      <td>[-1.2669662, -1.2965895, -1.2532939, -0.986684...</td>\n",
       "      <td>[0.28711826, 0.84996116, 1.0960625, 0.8226166,...</td>\n",
       "      <td>[-1.2556804, -1.2580429, -1.3312811, -1.118654...</td>\n",
       "      <td>[0.060244307, 0.4748669, 0.7654571, 0.6591436,...</td>\n",
       "      <td>[-1.9912907, -1.9041362, -1.9325562, -1.542255...</td>\n",
       "      <td>[0.69344664, 1.4001559, 1.7582471, 1.4304705, ...</td>\n",
       "      <td>[-0.19985186, -0.074944444, -0.18423842, -0.13...</td>\n",
       "      <td>[-0.59643286, 0.27167362, 0.6901134, 0.4371759...</td>\n",
       "      <td>[-0.32119048, -0.52872896, -0.96851283, -0.753...</td>\n",
       "      <td>[0.096357144, 0.88450915, 1.2131118, 0.8943919...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11310658</td>\n",
       "      <td>20221202_134600_omission_and_competition_subje...</td>\n",
       "      <td>20221202_134600_omission_and_competition_subje...</td>\n",
       "      <td>6390821</td>\n",
       "      <td>20221202_134600_omission_and_competition_subje...</td>\n",
       "      <td>7966</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6_1_top_2_base_3</td>\n",
       "      <td>omission</td>\n",
       "      <td>20221202_134600_omission_and_competition_subje...</td>\n",
       "      <td>...</td>\n",
       "      <td>[-2.0257788, -2.0348935, -1.9323514, -1.754611...</td>\n",
       "      <td>[2.376701, 2.3015034, 1.7796774, 0.9411098, 0....</td>\n",
       "      <td>[0.16655779, 0.42879772, 0.66268736, 0.6934002...</td>\n",
       "      <td>[-1.8427671, -2.303459, -2.6802812, -3.060647,...</td>\n",
       "      <td>[-1.2637402, -1.0382752, -0.82986236, -0.74649...</td>\n",
       "      <td>[2.6771586, 2.3929594, 2.209177, 1.9761335, 1....</td>\n",
       "      <td>[-2.538743, -2.1983705, -1.8673657, -1.7143542...</td>\n",
       "      <td>[2.8447661, 2.3045416, 1.5301157, 0.96490973, ...</td>\n",
       "      <td>[-2.7647088, -2.5546997, -2.3051593, -2.055619...</td>\n",
       "      <td>[2.087738, 1.7418406, 1.1266373, 0.45954946, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12810657</td>\n",
       "      <td>20221202_134600_omission_and_competition_subje...</td>\n",
       "      <td>20221202_134600_omission_and_competition_subje...</td>\n",
       "      <td>7890820</td>\n",
       "      <td>20221202_134600_omission_and_competition_subje...</td>\n",
       "      <td>9836</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6_1_top_2_base_3</td>\n",
       "      <td>rewarded</td>\n",
       "      <td>20221202_134600_omission_and_competition_subje...</td>\n",
       "      <td>...</td>\n",
       "      <td>[-0.5765152, 0.25749493, 0.6403192, 0.4375135,...</td>\n",
       "      <td>[-0.043295607, 0.73602533, 0.31674156, 0.07747...</td>\n",
       "      <td>[-0.31421542, 0.19727057, 0.4453354, 0.3744597...</td>\n",
       "      <td>[0.21617076, 0.8221576, 0.58236164, 0.43116024...</td>\n",
       "      <td>[-2.1352851, -2.0576038, -2.0822346, -2.140969...</td>\n",
       "      <td>[-0.18188764, 0.113679774, -0.66123736, -0.935...</td>\n",
       "      <td>[-2.1671436, -1.4832754, -1.0554676, -1.130412...</td>\n",
       "      <td>[0.5339792, 1.5113796, 0.57145137, -0.02810416...</td>\n",
       "      <td>[-2.0111465, -1.714663, -1.4255916, -1.3662949...</td>\n",
       "      <td>[0.31871977, 1.008044, 0.25942308, -0.22730403...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       time                                      recording_dir  \\\n",
       "0   6310663  20221202_134600_omission_and_competition_subje...   \n",
       "1   7910662  20221202_134600_omission_and_competition_subje...   \n",
       "2   9710660  20221202_134600_omission_and_competition_subje...   \n",
       "3  11310658  20221202_134600_omission_and_competition_subje...   \n",
       "4  12810657  20221202_134600_omission_and_competition_subje...   \n",
       "\n",
       "                                      recording_file  time_stamp_index  \\\n",
       "0  20221202_134600_omission_and_competition_subje...           1390826   \n",
       "1  20221202_134600_omission_and_competition_subje...           2990825   \n",
       "2  20221202_134600_omission_and_competition_subje...           4790823   \n",
       "3  20221202_134600_omission_and_competition_subje...           6390821   \n",
       "4  20221202_134600_omission_and_competition_subje...           7890820   \n",
       "\n",
       "                                          video_file  video_frame  \\\n",
       "0  20221202_134600_omission_and_competition_subje...         1734   \n",
       "1  20221202_134600_omission_and_competition_subje...         3728   \n",
       "2  20221202_134600_omission_and_competition_subje...         5972   \n",
       "3  20221202_134600_omission_and_competition_subje...         7966   \n",
       "4  20221202_134600_omission_and_competition_subje...         9836   \n",
       "\n",
       "   video_number      subject_info competition_closeness  \\\n",
       "0           1.0  6_1_top_2_base_3              rewarded   \n",
       "1           1.0  6_1_top_2_base_3              rewarded   \n",
       "2           1.0  6_1_top_2_base_3              rewarded   \n",
       "3           1.0  6_1_top_2_base_3              omission   \n",
       "4           1.0  6_1_top_2_base_3              rewarded   \n",
       "\n",
       "                                          video_name  ...  \\\n",
       "0  20221202_134600_omission_and_competition_subje...  ...   \n",
       "1  20221202_134600_omission_and_competition_subje...  ...   \n",
       "2  20221202_134600_omission_and_competition_subje...  ...   \n",
       "3  20221202_134600_omission_and_competition_subje...  ...   \n",
       "4  20221202_134600_omission_and_competition_subje...  ...   \n",
       "\n",
       "                             mPFC_baseline_lfp_trace  \\\n",
       "0  [1.8457601, 1.7363818, 1.6475118, 1.59738, 1.2...   \n",
       "1  [1.2191132, 1.1348007, 1.2054409, 1.0960625, 0...   \n",
       "2  [-1.2669662, -1.2965895, -1.2532939, -0.986684...   \n",
       "3  [-2.0257788, -2.0348935, -1.9323514, -1.754611...   \n",
       "4  [-0.5765152, 0.25749493, 0.6403192, 0.4375135,...   \n",
       "\n",
       "                                mPFC_trial_lfp_trace  \\\n",
       "0  [0.6927297, 0.96389693, 0.7884358, -0.04101689...   \n",
       "1  [1.0732753, 0.7246318, 0.7633699, 0.3782669, -...   \n",
       "2  [0.28711826, 0.84996116, 1.0960625, 0.8226166,...   \n",
       "3  [2.376701, 2.3015034, 1.7796774, 0.9411098, 0....   \n",
       "4  [-0.043295607, 0.73602533, 0.31674156, 0.07747...   \n",
       "\n",
       "                             vHPC_baseline_lfp_trace  \\\n",
       "0  [-0.06969439, -0.09568214, -0.05315674, 0.1571...   \n",
       "1  [0.31539667, 0.23152715, 0.29767776, 0.4217101...   \n",
       "2  [-1.2556804, -1.2580429, -1.3312811, -1.118654...   \n",
       "3  [0.16655779, 0.42879772, 0.66268736, 0.6934002...   \n",
       "4  [-0.31421542, 0.19727057, 0.4453354, 0.3744597...   \n",
       "\n",
       "                                vHPC_trial_lfp_trace  \\\n",
       "0  [1.5864334, 1.5710771, 1.5970649, 1.2155175, 0...   \n",
       "1  [0.03543783, -0.27641505, -0.40044746, -0.6638...   \n",
       "2  [0.060244307, 0.4748669, 0.7654571, 0.6591436,...   \n",
       "3  [-1.8427671, -2.303459, -2.6802812, -3.060647,...   \n",
       "4  [0.21617076, 0.8221576, 0.58236164, 0.43116024...   \n",
       "\n",
       "                              BLA_baseline_lfp_trace  \\\n",
       "0  [2.0367627, 2.1163385, 2.1618104, 2.2679114, 2...   \n",
       "1  [0.3107247, 0.14209972, -0.05873455, -0.331566...   \n",
       "2  [-1.9912907, -1.9041362, -1.9325562, -1.542255...   \n",
       "3  [-1.2637402, -1.0382752, -0.82986236, -0.74649...   \n",
       "4  [-2.1352851, -2.0576038, -2.0822346, -2.140969...   \n",
       "\n",
       "                                 BLA_trial_lfp_trace  \\\n",
       "0  [0.3164087, 0.36377528, 0.18757163, -0.5020857...   \n",
       "1  [0.026525281, -0.04547191, 0.11936376, -0.4092...   \n",
       "2  [0.69344664, 1.4001559, 1.7582471, 1.4304705, ...   \n",
       "3  [2.6771586, 2.3929594, 2.209177, 1.9761335, 1....   \n",
       "4  [-0.18188764, 0.113679774, -0.66123736, -0.935...   \n",
       "\n",
       "                               LH_baseline_lfp_trace  \\\n",
       "0  [3.1382985, 3.2319791, 3.2788196, 3.2881875, 3...   \n",
       "1  [-1.180375, -1.2959143, -1.3771042, -1.458294,...   \n",
       "2  [-0.19985186, -0.074944444, -0.18423842, -0.13...   \n",
       "3  [-2.538743, -2.1983705, -1.8673657, -1.7143542...   \n",
       "4  [-2.1671436, -1.4832754, -1.0554676, -1.130412...   \n",
       "\n",
       "                                  LH_trial_lfp_trace  \\\n",
       "0  [0.8118982, 1.2209699, 0.87435186, -0.4028264,...   \n",
       "1  [0.9492963, 0.46840277, 0.6713773, 0.043717593...   \n",
       "2  [-0.59643286, 0.27167362, 0.6901134, 0.4371759...   \n",
       "3  [2.8447661, 2.3045416, 1.5301157, 0.96490973, ...   \n",
       "4  [0.5339792, 1.5113796, 0.57145137, -0.02810416...   \n",
       "\n",
       "                               MD_baseline_lfp_trace  \\\n",
       "0  [1.3934726, 1.494771, 1.764077, 1.828315, 1.68...   \n",
       "1  [-0.14577106, -0.16059524, 0.027177656, 0.1680...   \n",
       "2  [-0.32119048, -0.52872896, -0.96851283, -0.753...   \n",
       "3  [-2.7647088, -2.5546997, -2.3051593, -2.055619...   \n",
       "4  [-2.0111465, -1.714663, -1.4255916, -1.3662949...   \n",
       "\n",
       "                                  MD_trial_lfp_trace  \n",
       "0  [-0.9783956, -0.86721426, -0.7288553, -1.40582...  \n",
       "1  [1.6281886, 1.349, 1.4675934, 0.9487473, -0.21...  \n",
       "2  [0.096357144, 0.88450915, 1.2131118, 0.8943919...  \n",
       "3  [2.087738, 1.7418406, 1.1266373, 0.45954946, 0...  \n",
       "4  [0.31871977, 1.008044, 0.25942308, -0.22730403...  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BASIC_LFP_TRACES_DF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_columns = [col for col in BASIC_LFP_TRACES_DF.columns if \"trace\" in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_cols = [col for col in BASIC_LFP_TRACES_DF.columns if col not in trace_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mPFC_baseline_lfp_trace',\n",
       " 'mPFC_trial_lfp_trace',\n",
       " 'vHPC_baseline_lfp_trace',\n",
       " 'vHPC_trial_lfp_trace',\n",
       " 'BLA_baseline_lfp_trace',\n",
       " 'BLA_trial_lfp_trace',\n",
       " 'LH_baseline_lfp_trace',\n",
       " 'LH_trial_lfp_trace',\n",
       " 'MD_baseline_lfp_trace',\n",
       " 'MD_trial_lfp_trace']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASIC_LFP_TRACES_DF = BASIC_LFP_TRACES_DF.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coherece Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Getting the brain region pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_trace_columns = sorted([col for col in BASIC_LFP_TRACES_DF.columns if \"trial_lfp_trace\" in col])\n",
    "baseline_trace_columns = sorted([col for col in BASIC_LFP_TRACES_DF.columns if \"baseline_lfp_trace\" in col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BLA_trial_lfp_trace',\n",
       " 'LH_trial_lfp_trace',\n",
       " 'MD_trial_lfp_trace',\n",
       " 'mPFC_trial_lfp_trace',\n",
       " 'vHPC_trial_lfp_trace']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_trace_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_brain_region_pairs = generate_pairs(trial_trace_columns)\n",
    "trial_brain_region_pairs = sorted(trial_brain_region_pairs)\n",
    "baseline_brain_region_pairs = generate_pairs(baseline_trace_columns)\n",
    "baseline_brain_region_pairs = sorted(baseline_brain_region_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('BLA_trial_lfp_trace', 'LH_trial_lfp_trace'),\n",
       " ('BLA_trial_lfp_trace', 'MD_trial_lfp_trace'),\n",
       " ('BLA_trial_lfp_trace', 'mPFC_trial_lfp_trace'),\n",
       " ('BLA_trial_lfp_trace', 'vHPC_trial_lfp_trace'),\n",
       " ('LH_trial_lfp_trace', 'MD_trial_lfp_trace'),\n",
       " ('LH_trial_lfp_trace', 'mPFC_trial_lfp_trace'),\n",
       " ('LH_trial_lfp_trace', 'vHPC_trial_lfp_trace'),\n",
       " ('MD_trial_lfp_trace', 'mPFC_trial_lfp_trace'),\n",
       " ('MD_trial_lfp_trace', 'vHPC_trial_lfp_trace'),\n",
       " ('mPFC_trial_lfp_trace', 'vHPC_trial_lfp_trace')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_brain_region_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('BLA_baseline_lfp_trace', 'LH_baseline_lfp_trace'),\n",
       " ('BLA_baseline_lfp_trace', 'MD_baseline_lfp_trace'),\n",
       " ('BLA_baseline_lfp_trace', 'mPFC_baseline_lfp_trace'),\n",
       " ('BLA_baseline_lfp_trace', 'vHPC_baseline_lfp_trace'),\n",
       " ('LH_baseline_lfp_trace', 'MD_baseline_lfp_trace'),\n",
       " ('LH_baseline_lfp_trace', 'mPFC_baseline_lfp_trace'),\n",
       " ('LH_baseline_lfp_trace', 'vHPC_baseline_lfp_trace'),\n",
       " ('MD_baseline_lfp_trace', 'mPFC_baseline_lfp_trace'),\n",
       " ('MD_baseline_lfp_trace', 'vHPC_baseline_lfp_trace'),\n",
       " ('mPFC_baseline_lfp_trace', 'vHPC_baseline_lfp_trace')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_brain_region_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Calculating the granger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLA_LH\n",
      "\tcalculating connectivity\n",
      "\tBLA_LH_granger\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 0 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 7 of 19 converged\n",
      "Maximum iterations reached. 0 of 19 converged\n",
      "Maximum iterations reached. 0 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLH_BLA_granger\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 0 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 7 of 19 converged\n",
      "Maximum iterations reached. 0 of 19 converged\n",
      "Maximum iterations reached. 0 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLA_MD\n",
      "\tcalculating connectivity\n",
      "\tBLA_MD_granger\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 17 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 0 of 19 converged\n",
      "Maximum iterations reached. 5 of 19 converged\n",
      "Maximum iterations reached. 0 of 19 converged\n",
      "Maximum iterations reached. 0 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tMD_BLA_granger\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 17 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 0 of 19 converged\n",
      "Maximum iterations reached. 5 of 19 converged\n",
      "Maximum iterations reached. 0 of 19 converged\n",
      "Maximum iterations reached. 0 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLA_mPFC\n",
      "\tcalculating connectivity\n",
      "\tBLA_mPFC_granger\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 0 of 19 converged\n",
      "Maximum iterations reached. 5 of 19 converged\n",
      "Maximum iterations reached. 0 of 19 converged\n",
      "Maximum iterations reached. 0 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tmPFC_BLA_granger\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 0 of 19 converged\n",
      "Maximum iterations reached. 5 of 19 converged\n",
      "Maximum iterations reached. 0 of 19 converged\n",
      "Maximum iterations reached. 0 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLA_vHPC\n",
      "\tcalculating connectivity\n",
      "\tBLA_vHPC_granger\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Maximum iterations reached. 0 of 19 converged\n",
      "Maximum iterations reached. 6 of 19 converged\n",
      "Maximum iterations reached. 0 of 19 converged\n",
      "Maximum iterations reached. 0 of 19 converged\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvHPC_BLA_granger\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Maximum iterations reached. 0 of 19 converged\n",
      "Maximum iterations reached. 6 of 19 converged\n",
      "Maximum iterations reached. 0 of 19 converged\n",
      "Maximum iterations reached. 0 of 19 converged\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LH_MD\n",
      "\tcalculating connectivity\n",
      "\tLH_MD_granger\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 0 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 3 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 17 of 19 converged\n",
      "Maximum iterations reached. 0 of 19 converged\n",
      "Maximum iterations reached. 0 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 17 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tMD_LH_granger\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 0 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 3 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 17 of 19 converged\n",
      "Maximum iterations reached. 0 of 19 converged\n",
      "Maximum iterations reached. 0 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 17 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LH_mPFC\n",
      "\tcalculating connectivity\n",
      "\tLH_mPFC_granger\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 0 of 19 converged\n",
      "Maximum iterations reached. 4 of 19 converged\n",
      "Maximum iterations reached. 0 of 19 converged\n",
      "Maximum iterations reached. 0 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tmPFC_LH_granger\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 0 of 19 converged\n",
      "Maximum iterations reached. 4 of 19 converged\n",
      "Maximum iterations reached. 0 of 19 converged\n",
      "Maximum iterations reached. 0 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LH_vHPC\n",
      "\tcalculating connectivity\n",
      "\tLH_vHPC_granger\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Maximum iterations reached. 0 of 19 converged\n",
      "Maximum iterations reached. 17 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 17 of 19 converged\n",
      "Maximum iterations reached. 3 of 19 converged\n",
      "Maximum iterations reached. 0 of 19 converged\n",
      "Maximum iterations reached. 0 of 19 converged\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvHPC_LH_granger\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Maximum iterations reached. 0 of 19 converged\n",
      "Maximum iterations reached. 17 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 17 of 19 converged\n",
      "Maximum iterations reached. 3 of 19 converged\n",
      "Maximum iterations reached. 0 of 19 converged\n",
      "Maximum iterations reached. 0 of 19 converged\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MD_mPFC\n",
      "\tcalculating connectivity\n",
      "\tMD_mPFC_granger\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 0 of 19 converged\n",
      "Maximum iterations reached. 6 of 19 converged\n",
      "Maximum iterations reached. 0 of 19 converged\n",
      "Maximum iterations reached. 0 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tmPFC_MD_granger\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 18 of 19 converged\n",
      "Maximum iterations reached. 0 of 19 converged\n",
      "Maximum iterations reached. 6 of 19 converged\n",
      "Maximum iterations reached. 0 of 19 converged\n",
      "Maximum iterations reached. 0 of 19 converged\n"
     ]
    }
   ],
   "source": [
    "for region_1, region_2 in trial_brain_region_pairs:\n",
    "    region_1_trimmed = region_1.split(\"_\")[0]\n",
    "    region_2_trimmed = region_2.split(\"_\")[0]\n",
    "    pair_base_name = \"{}_{}\".format(region_1_trimmed, region_2_trimmed)\n",
    "    print(pair_base_name)\n",
    "    try:\n",
    "        multitaper_col = \"{}_trial_multitaper\".format(pair_base_name)\n",
    "        # BASIC_LFP_TRACES_DF[multitaper_col] = BASIC_LFP_TRACES_DF.apply(lambda x: Multitaper(time_series=np.array([x[region_1],x[region_2]]).T, sampling_frequency=RESAMPLE_RATE, time_halfbandwidth_product=TIME_HALFBANDWIDTH_PRODUCT), axis=1)\n",
    "        BASIC_LFP_TRACES_DF[multitaper_col] = BASIC_LFP_TRACES_DF.apply(lambda x: Multitaper(time_series=np.array([x[region_1],x[region_2]]).T, sampling_frequency=RESAMPLE_RATE, time_halfbandwidth_product=TIME_HALFBANDWIDTH_PRODUCT, time_window_duration=TIME_WINDOW_DURATION, time_window_step=TIME_WINDOW_STEP), axis=1)\n",
    "        print(\"\\tcalculating connectivity\")\n",
    "        connectivity_col = \"{}_trial_connectivity\".format(pair_base_name)\n",
    "        BASIC_LFP_TRACES_DF[connectivity_col] = BASIC_LFP_TRACES_DF[multitaper_col].apply(lambda x: Connectivity.from_multitaper(x))\n",
    "        \n",
    "        BASIC_LFP_TRACES_DF[\"{}_trial_frequencies\".format(pair_base_name)] = BASIC_LFP_TRACES_DF[connectivity_col].apply(lambda x: x.frequencies)\n",
    "        \n",
    "        print(\"\\t{}_{}_granger\".format(region_1_trimmed, region_2_trimmed))\n",
    "        BASIC_LFP_TRACES_DF[\"{}_{}_split_windows_granger\".format(region_1_trimmed, region_2_trimmed)] = BASIC_LFP_TRACES_DF[connectivity_col].apply(lambda x: x.pairwise_spectral_granger_prediction()[:,:,0,1])\n",
    "\n",
    "        BASIC_LFP_TRACES_DF[\"{}_{}_window_mean_granger\".format(region_1_trimmed, region_2_trimmed)] = BASIC_LFP_TRACES_DF[\"{}_{}_split_windows_granger\".format(region_1_trimmed, region_2_trimmed)].apply(lambda x: np.nanmean(x, axis=0))\n",
    "\n",
    "        print(\"\\t{}_{}_granger\".format(region_2_trimmed, region_1_trimmed))\n",
    "        BASIC_LFP_TRACES_DF[\"{}_{}_split_windows_granger\".format(region_2_trimmed, region_1_trimmed)] = BASIC_LFP_TRACES_DF[connectivity_col].apply(lambda x: x.pairwise_spectral_granger_prediction()[:,:,1,0])\n",
    "\n",
    "        BASIC_LFP_TRACES_DF[\"{}_{}_window_mean_granger\".format(region_2_trimmed, region_1_trimmed)] = BASIC_LFP_TRACES_DF[\"{}_{}_split_windows_granger\".format(region_2_trimmed, region_1_trimmed)].apply(lambda x: np.nanmean(x, axis=0))\n",
    "        \n",
    "        BASIC_LFP_TRACES_DF = BASIC_LFP_TRACES_DF.drop(columns=[connectivity_col], errors=\"ignore\")\n",
    "        BASIC_LFP_TRACES_DF.to_pickle(\"./proc/per_trial_granger.pkl\")\n",
    "    except Exception as e: \n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASIC_LFP_TRACES_DF[\"BLA_LH_trial_frequencies\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASIC_LFP_TRACES_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GROUPINGS = \"trial_outcome\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_frequencies = BASIC_LFP_TRACES_DF[\"BLA_LH_trial_frequencies\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FREQ_MAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "low_freq = 0\n",
    "high_freq = 100\n",
    "\n",
    "for region_1, region_2 in trial_brain_region_pairs:\n",
    "    region_1_trimmed = region_1.split(\"_\")[0]\n",
    "    region_2_trimmed = region_2.split(\"_\")[0]\n",
    "    \n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    plt.title(\"granger of Z-scored LFP in {} to {}\".format(region_1_trimmed, region_2_trimmed))\n",
    "    plt.xlim(low_freq, high_freq)\n",
    "    plt.ylim(0,1)\n",
    "\n",
    "    ax.axvspan(4, 12, alpha=0.1, color='yellow', label=\"theta\")\n",
    "    ax.axvspan(12, 30, alpha=0.1, color='cyan', label=\"beta\")\n",
    "    ax.axvspan(30, 90, alpha=0.1, color='magenta', label=\"gamma\")\n",
    "    \n",
    "    granger_col = \"{}_{}_window_mean_granger\".format(region_1_trimmed, region_2_trimmed)\n",
    "    grouped_all_trials_df = BASIC_LFP_TRACES_DF.groupby([GROUPINGS]).agg({granger_col: lambda x: np.vstack(x.tolist())}).reset_index()\n",
    "    # grouped_all_trials_df = grouped_all_trials_df[grouped_all_trials_df[\"trial_or_baseline\"] == \"trial\"]\n",
    "    grouped_all_trials_df[\"mean_granger\"] = grouped_all_trials_df[granger_col].apply(lambda x: np.nanmean(np.vstack(x), axis=0))\n",
    "    grouped_all_trials_df[\"std_granger\"] = grouped_all_trials_df[granger_col].apply(lambda x: np.nanstd(np.vstack(x), axis=0))\n",
    "    grouped_all_trials_df[\"n_trials\"] = grouped_all_trials_df[granger_col].apply(lambda x: np.sum(~np.isnan(x), axis=0))\n",
    "    grouped_all_trials_df[\"sem_granger\"] = grouped_all_trials_df.apply(lambda x: x[\"std_granger\"] / np.sqrt(x[\"n_trials\"]), axis=1)\n",
    "    for index, row in grouped_all_trials_df.iterrows():\n",
    "        try:\n",
    "\n",
    "            ax = sns.lineplot(x=all_frequencies, y=row[\"mean_granger\"], \\\n",
    "            label=\"{}\".format(row[GROUPINGS]), color=BASELINE_OUTCOME_TO_COLOR[row[GROUPINGS]],\n",
    "            linewidth=3)\n",
    "\n",
    "    \n",
    "    \n",
    "            plt.fill_between(all_frequencies, \\\n",
    "            row[\"mean_granger\"] - row[\"sem_granger\"], row[\"mean_granger\"] + row[\"sem_granger\"], alpha=0.1,\n",
    "            color=BASELINE_OUTCOME_TO_COLOR[row[GROUPINGS]])\n",
    "        except Exception as e: \n",
    "            print(e)\n",
    "\n",
    "    plt.legend(ncol=3, loc=\"lower center\")\n",
    "\n",
    "    plt.savefig(\"./proc/{}_{}_granger_{}_{}hz.png\".format(region_1_trimmed, region_2_trimmed, low_freq, high_freq))\n",
    "    plt.show()\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    plt.title(\"granger of Z-scored LFP in {} to {}\".format(region_2_trimmed, region_1_trimmed))\n",
    "    plt.xlim(low_freq, high_freq)\n",
    "    plt.ylim(0,1)\n",
    "\n",
    "    ax.axvspan(4, 12, alpha=0.1, color='yellow', label=\"theta\")\n",
    "    ax.axvspan(12, 30, alpha=0.1, color='cyan', label=\"beta\")\n",
    "    ax.axvspan(30, 90, alpha=0.1, color='magenta', label=\"gamma\")\n",
    "    \n",
    "    granger_col = \"{}_{}_window_mean_granger\".format(region_2_trimmed, region_1_trimmed)\n",
    "    grouped_all_trials_df = BASIC_LFP_TRACES_DF.groupby([GROUPINGS]).agg({granger_col: lambda x: np.vstack(x.tolist())}).reset_index()\n",
    "    # grouped_all_trials_df = grouped_all_trials_df[grouped_all_trials_df[\"trial_or_baseline\"] == \"trial\"]\n",
    "    grouped_all_trials_df[\"mean_granger\"] = grouped_all_trials_df[granger_col].apply(lambda x: np.nanmean(np.vstack(x), axis=0))\n",
    "    grouped_all_trials_df[\"std_granger\"] = grouped_all_trials_df[granger_col].apply(lambda x: np.nanstd(np.vstack(x), axis=0))\n",
    "    grouped_all_trials_df[\"n_trials\"] = grouped_all_trials_df[granger_col].apply(lambda x: np.sum(~np.isnan(x), axis=0))\n",
    "    grouped_all_trials_df[\"sem_granger\"] = grouped_all_trials_df.apply(lambda x: x[\"std_granger\"] / np.sqrt(x[\"n_trials\"]), axis=1)\n",
    "    for index, row in grouped_all_trials_df.iterrows():\n",
    "        try:\n",
    "\n",
    "            ax = sns.lineplot(x=all_frequencies, y=row[\"mean_granger\"], \\\n",
    "            label=\"{}\".format(row[GROUPINGS]), color=BASELINE_OUTCOME_TO_COLOR[row[GROUPINGS]],\n",
    "            linewidth=3)\n",
    "\n",
    "    \n",
    "    \n",
    "            plt.fill_between(all_frequencies, \\\n",
    "            row[\"mean_granger\"] - row[\"sem_granger\"], row[\"mean_granger\"] + row[\"sem_granger\"], alpha=0.1,\n",
    "            color=BASELINE_OUTCOME_TO_COLOR[row[GROUPINGS]])\n",
    "        except Exception as e: \n",
    "            print(e)\n",
    "    \n",
    "    plt.legend(ncol=3, loc=\"lower center\")\n",
    "\n",
    "    plt.savefig(\"./proc/{}_{}_granger_{}_{}hz.png\".format(region_2_trimmed, region_1_trimmed, low_freq, high_freq))\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_frequencies[4:13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FREQUENCY_BANDS = (4,13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BAND_TO_FREQUENCY = {\"theta\": (4,13), \"beta\": (12,31), \"gamma\": (30, 91)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BAND_TO_ALL_COL = {\"theta\": [], \"beta\": [], \"gamma\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BASIC_LFP_TRACES_DF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_granger_col = [col for col in BASIC_LFP_TRACES_DF.columns if \"window_mean_granger\" in col]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_granger_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for band, frequency in BAND_TO_FREQUENCY.items():\n",
    "    for col in all_granger_col:\n",
    "        BAND_COL = \"{}_averaged_{}_granger\".format(\"_\".join(col.split(\"_\")[:2]), band)\n",
    "        BAND_TO_ALL_COL[band].append(BAND_COL)\n",
    "        print(BAND_COL)\n",
    "        print(frequency[0], frequency[1])\n",
    "        BASIC_LFP_TRACES_DF[BAND_COL] = BASIC_LFP_TRACES_DF[col].apply(lambda x: np.nanmean(x[frequency[0]:frequency[1]]))    \n",
    "        # BASIC_LFP_TRACES_DF[BAND_COL] = BASIC_LFP_TRACES_DF[col].apply(lambda x: x[frequency[0]:frequency[1]])    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASIC_LFP_TRACES_DF[\"BLA_LH_averaged_theta_granger\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASIC_LFP_TRACES_DF[col].iloc[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASIC_LFP_TRACES_DF[GROUPINGS].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_pair_to_outcome_to_granger = {k: defaultdict(nested_dict) for k,v in BAND_TO_FREQUENCY.items()}\n",
    "\n",
    "for band, frequency in BAND_TO_FREQUENCY.items():\n",
    "    for outcome in BASIC_LFP_TRACES_DF[GROUPINGS].unique():\n",
    "        outcome_df = BASIC_LFP_TRACES_DF[BASIC_LFP_TRACES_DF[GROUPINGS] == outcome].copy()\n",
    "        for band_col in set(BAND_TO_ALL_COL[band]):\n",
    "            region_pair_to_outcome_to_granger[band][\"_\".join(band_col.split(\"_\")[:-3])][outcome][\"mean\"] = outcome_df[band_col].mean() \n",
    "            region_pair_to_outcome_to_granger[band][\"_\".join(band_col.split(\"_\")[:-3])][outcome][\"sem\"] = outcome_df[band_col].sem() \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BAND_TO_ALL_COL[band]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for band, frequency in BAND_TO_FREQUENCY.items(): \n",
    "    # Convert the nested dictionary to a DataFrame\n",
    "    data = []\n",
    "    for group_name, group_data in region_pair_to_outcome_to_granger[band].items():\n",
    "        for bar_name, bar_dict in group_data.items():\n",
    "            data.append({\"Group\": group_name, \"Bar\": bar_name, \"granger\": bar_dict[\"mean\"], \"sem\": bar_dict[\"sem\"]})\n",
    "    df = pd.DataFrame(data).sort_values(by=[\"Group\", \"Bar\"])\n",
    "    df[\"color\"] = df[\"Bar\"].map(BASELINE_OUTCOME_TO_COLOR)\n",
    "    \n",
    "    # Create barplot\n",
    "    ax = sns.barplot(x='Group', y='granger', hue='Bar', data=df, palette=df[\"color\"], ci=None)\n",
    "    \n",
    "    # Adding error bars\n",
    "    groups = df['Group'].unique()\n",
    "    bars_per_group = df['Bar'].nunique()\n",
    "    bar_width = 0.8 / bars_per_group\n",
    "    x_positions = []\n",
    "    \n",
    "    for i, group in enumerate(groups):\n",
    "        num_bars = df[df['Group'] == group].shape[0]\n",
    "        group_positions = np.linspace(i - bar_width*(num_bars-1)/2, i + bar_width*(num_bars-1)/2, num_bars)\n",
    "        x_positions.extend(group_positions)\n",
    "\n",
    "        for pos, bar in zip(group_positions, group_data['Bar']):\n",
    "            height = group_data[group_data['Bar'] == bar]['granger']\n",
    "            error = group_data[group_data['Bar'] == bar]['sem']\n",
    "            plt.errorbar(pos, height, yerr=error, color='black', capsize=3, fmt='none', zorder=10)\n",
    "\n",
    "    \n",
    "    plt.xticks(rotation=90)\n",
    "    plt.xlabel(\"Brain region pairs\")\n",
    "    plt.ylabel(\"Averaged {} granger\".format(band))\n",
    "    plt.legend(title=\"Trial Conditions\", loc=\"lower left\", ncol=4)\n",
    "    plt.title(\"Averaged {} granger\".format(band))\n",
    "    plt.tight_layout()\n",
    "    plt.grid()\n",
    "    plt.ylim(0,1)\n",
    "    \n",
    "    # plt.savefig(\"./proc/granger/all_zscored_{}_lfp_power_granger.png\".format(band))\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "font = {'weight' : 'bold',\n",
    "        'size'   : 12}\n",
    "\n",
    "matplotlib.rc('font', **font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for band, frequency in BAND_TO_FREQUENCY.items(): \n",
    "    # Convert the nested dictionary to a DataFrame\n",
    "    data = []\n",
    "    for group_name, group_data in region_pair_to_outcome_to_granger[band].items():\n",
    "        for bar_name, bar_dict in group_data.items():\n",
    "            data.append({\"Group\": group_name, \"Bar\": bar_name, \"granger\": bar_dict[\"mean\"], \"sem\": bar_dict[\"sem\"]})\n",
    "    df = pd.DataFrame(data).sort_values(by=[\"Group\", \"Bar\"])\n",
    "    df[\"color\"] = df[\"Bar\"].map(BASELINE_OUTCOME_TO_COLOR)    \n",
    "    # Create barplot\n",
    "    ax = sns.barplot(x='Group', y='granger', hue='Bar', data=df, palette=df[\"color\"], ci=None)\n",
    "    \n",
    "    # Adding error bars\n",
    "    groups = df['Group'].unique()\n",
    "    bars_per_group = df['Bar'].nunique()\n",
    "    bar_width = 0.8 / bars_per_group  # the width of the bars\n",
    "    x_positions = []  # this will store the x positions for the error bars\n",
    "\n",
    "    for i, group in enumerate(groups):\n",
    "        group_data = df[df['Group'] == group]  # filter the dataframe for the specific group\n",
    "        num_bars = group_data.shape[0]  # get the number of bars for this group\n",
    "        group_positions = np.linspace(i - bar_width*(num_bars-1)/2, i + bar_width*(num_bars-1)/2, num_bars)\n",
    "        x_positions.extend(group_positions)  # add these positions to the list\n",
    "\n",
    "        for pos, (idx, row) in zip(group_positions, group_data.iterrows()):\n",
    "            height = row['granger']\n",
    "            error = row['sem']\n",
    "            plt.errorbar(pos, height, yerr=error, color='black', capsize=3, fmt='none', zorder=10)\n",
    "    \n",
    "    plt.xticks(rotation=90)\n",
    "    plt.xlabel(\"Brain region pairs\", fontsize=20)\n",
    "    plt.ylabel(\"{} granger\".format(band.title()), fontsize=20)\n",
    "    plt.legend(title=\"Trial Conditions\", loc=\"lower left\", ncol=4)\n",
    "    plt.title(\"{} granger\".format(band.title()), fontsize=20)\n",
    "    plt.tight_layout()\n",
    "    plt.grid()\n",
    "    plt.ylim(0,1)\n",
    "    ax.get_legend().remove()\n",
    "\n",
    "    plt.savefig(\"./proc/granger/all_zscored_{}_lfp_power_granger.png\".format(band))\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_outcome_pairs = (\"win\", \"lose\"), (\"win\", \"rewarded\"), (\"lose\", \"omission\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for band, frequency in BAND_TO_FREQUENCY.items():\n",
    "    print(band)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASIC_LFP_TRACES_DF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASIC_LFP_TRACES_DF[\"trial_outcome\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_to_mannwhitneyu = defaultdict(dict)\n",
    "region_to_mannwhitneyu = []\n",
    "\n",
    "for band, frequency in BAND_TO_FREQUENCY.items():\n",
    "    print(band)\n",
    "    for region_col in [col for col in BASIC_LFP_TRACES_DF.columns if band in col]:\n",
    "        print(region_col)  \n",
    "        for outcome_pair in trial_outcome_pairs:\n",
    "            outcome_1_granger = BASIC_LFP_TRACES_DF[BASIC_LFP_TRACES_DF[\"trial_outcome\"] == outcome_pair[0]][region_col].dropna()\n",
    "            outcome_2_granger = BASIC_LFP_TRACES_DF[BASIC_LFP_TRACES_DF[\"trial_outcome\"] == outcome_pair[1]][region_col].dropna()\n",
    "            statistic, p_value = mannwhitneyu(outcome_1_granger, outcome_2_granger, alternative='two-sided')\n",
    "            region_to_mannwhitneyu.append({\"band\": band, \"region\": region_col, \"p_value\": p_value, \"statistic\": statistic, \"outcome\": outcome_pair})\n",
    "            \n",
    "            # region_to_mannwhitneyu[region_col][outcome_pair] = p_value\n",
    "            # print(region_col)\n",
    "    \n",
    "    \n",
    "    # for outcome in BASIC_LFP_TRACES_DF[GROUPINGS].unique():\n",
    "    #     outcome_df = BASIC_LFP_TRACES_DF[BASIC_LFP_TRACES_DF[GROUPINGS] == outcome].copy()\n",
    "    #     for band_col in BAND_TO_ALL_COL[band]:\n",
    "    #         region_pair_to_outcome_to_granger[band][\"_\".join(band_col.split(\"_\")[:-3])][outcome][\"mean\"] = outcome_df[band_col].mean() \n",
    "    #         region_pair_to_outcome_to_granger[band][\"_\".join(band_col.split(\"_\")[:-3])][outcome][\"sem\"] = outcome_df[band_col].sem() \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "granger_mannwhitneyu = pd.DataFrame(region_to_mannwhitneyu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "granger_mannwhitneyu[granger_mannwhitneyu[\"p_value\"] <= 0.001/3].sort_values([\"p_value\", \"region\"], ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASIC_LFP_TRACES_DF.to_pickle(\"./proc/per_trial_granger.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise ValueError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GROUPINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASIC_LFP_TRACES_DF[\"BLA_LH_split_windows_granger\"].iloc[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_freq = 30\n",
    "high_freq = 90\n",
    "\n",
    "for region_1, region_2 in trial_brain_region_pairs:\n",
    "    region_1_trimmed = region_1.split(\"_\")[0]\n",
    "    region_2_trimmed = region_2.split(\"_\")[0]\n",
    "    \n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    plt.title(\"granger of Z-scored LFP in {} to {}\".format(region_1_trimmed, region_2_trimmed))\n",
    "    plt.xlim(low_freq, high_freq)\n",
    "    # plt.ylim(0,1)\n",
    "    \n",
    "    granger_col = \"{}_{}_window_mean_granger\".format(region_1_trimmed, region_2_trimmed)\n",
    "    grouped_all_trials_df = BASIC_LFP_TRACES_DF.groupby([GROUPINGS]).agg({granger_col: lambda x: np.vstack(x.tolist())}).reset_index()\n",
    "    # grouped_all_trials_df = grouped_all_trials_df[grouped_all_trials_df[\"trial_or_baseline\"] == \"trial\"]\n",
    "    grouped_all_trials_df[\"mean_granger\"] = grouped_all_trials_df[granger_col].apply(lambda x: np.nanmean(np.vstack(x), axis=0))\n",
    "    grouped_all_trials_df[\"std_granger\"] = grouped_all_trials_df[granger_col].apply(lambda x: np.nanstd(np.vstack(x), axis=0))\n",
    "    grouped_all_trials_df[\"n_trials\"] = grouped_all_trials_df[granger_col].apply(lambda x: np.sum(~np.isnan(x), axis=0))\n",
    "    grouped_all_trials_df[\"sem_granger\"] = grouped_all_trials_df.apply(lambda x: x[\"std_granger\"] / np.sqrt(x[\"n_trials\"]), axis=1)\n",
    "    for index, row in grouped_all_trials_df.iterrows():\n",
    "        try:\n",
    "\n",
    "            ax = sns.lineplot(x=all_frequencies, y=row[\"mean_granger\"], \\\n",
    "            label=\"{}\".format(row[GROUPINGS]), color=BASELINE_OUTCOME_TO_COLOR[row[GROUPINGS]],\n",
    "            linewidth=3)\n",
    "\n",
    "    \n",
    "    \n",
    "            plt.fill_between(all_frequencies, \\\n",
    "            row[\"mean_granger\"] - row[\"sem_granger\"], row[\"mean_granger\"] + row[\"sem_granger\"], alpha=0.1,\n",
    "            color=BASELINE_OUTCOME_TO_COLOR[row[GROUPINGS]])\n",
    "        except Exception as e: \n",
    "            print(e)\n",
    "    # plt.savefig(\"./proc/{}_{}_granger_{}_{}hz.png\".format(region_1_trimmed, region_2_trimmed, low_freq, high_freq))\n",
    "    plt.show()\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    plt.title(\"granger of Z-scored LFP in {} to {}\".format(region_2_trimmed, region_1_trimmed))\n",
    "    plt.xlim(low_freq, high_freq)\n",
    "    # plt.ylim(0,1)\n",
    "\n",
    "\n",
    "    \n",
    "    granger_col = \"{}_{}_window_mean_granger\".format(region_2_trimmed, region_1_trimmed)\n",
    "    grouped_all_trials_df = BASIC_LFP_TRACES_DF.groupby([GROUPINGS]).agg({granger_col: lambda x: np.vstack(x.tolist())}).reset_index()\n",
    "    # grouped_all_trials_df = grouped_all_trials_df[grouped_all_trials_df[\"trial_or_baseline\"] == \"trial\"]\n",
    "    grouped_all_trials_df[\"mean_granger\"] = grouped_all_trials_df[granger_col].apply(lambda x: np.nanmean(np.vstack(x), axis=0))\n",
    "    grouped_all_trials_df[\"std_granger\"] = grouped_all_trials_df[granger_col].apply(lambda x: np.nanstd(np.vstack(x), axis=0))\n",
    "    grouped_all_trials_df[\"n_trials\"] = grouped_all_trials_df[granger_col].apply(lambda x: np.sum(~np.isnan(x), axis=0))\n",
    "    grouped_all_trials_df[\"sem_granger\"] = grouped_all_trials_df.apply(lambda x: x[\"std_granger\"] / np.sqrt(x[\"n_trials\"]), axis=1)\n",
    "    for index, row in grouped_all_trials_df.iterrows():\n",
    "        try:\n",
    "\n",
    "            ax = sns.lineplot(x=all_frequencies, y=row[\"mean_granger\"], \\\n",
    "            label=\"{}\".format(row[GROUPINGS]), color=BASELINE_OUTCOME_TO_COLOR[row[GROUPINGS]],\n",
    "            linewidth=3)\n",
    "\n",
    "    \n",
    "    \n",
    "            plt.fill_between(all_frequencies, \\\n",
    "            row[\"mean_granger\"] - row[\"sem_granger\"], row[\"mean_granger\"] + row[\"sem_granger\"], alpha=0.1,\n",
    "            color=BASELINE_OUTCOME_TO_COLOR[row[GROUPINGS]])\n",
    "        except Exception as e: \n",
    "            print(e)\n",
    "    # plt.savefig(\"./proc/{}_{}_granger_{}_{}hz.png\".format(region_2_trimmed, region_1_trimmed, low_freq, high_freq))\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise ValueError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_all_trials_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_base_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_freq =30\n",
    "high_freq = 90\n",
    "for pair_base_name in all_pair_base_name:\n",
    "    print(pair_base_name)\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    plt.title(\"granger of Z-scored LFP in {}\".format(pair_base_name))\n",
    "    plt.xlabel(\"Frequency\")\n",
    "    plt.ylabel(\"granger of Z-scored LFP\")\n",
    "    plt.xlim(low_freq, high_freq)\n",
    "    plt.xticks(np.arange(low_freq, high_freq+1, 5))\n",
    "    plt.yticks(np.arange(0, 1, 0.1))\n",
    "    plt.ylim(0,1)\n",
    "    plt.grid()\n",
    "\n",
    "    for trial_or_baseline in TRIAL_AND_BASELINE:\n",
    "\n",
    "        granger_col = \"{}_{}_granger_magnitude\".format(pair_base_name, trial_or_baseline)\n",
    "        grouped_all_trials_df = channel_map_and_all_trials_df.groupby([GROUPINGS]).agg({granger_col: lambda x: np.vstack(x.tolist())}).reset_index()\n",
    "        # grouped_all_trials_df = grouped_all_trials_df[grouped_all_trials_df[\"trial_or_baseline\"] == \"trial\"]\n",
    "        grouped_all_trials_df[\"mean_granger\"] = grouped_all_trials_df[granger_col].apply(lambda x: np.nanmean(np.vstack(x), axis=0))\n",
    "        grouped_all_trials_df[\"std_granger\"] = grouped_all_trials_df[granger_col].apply(lambda x: np.nanstd(np.vstack(x), axis=0))\n",
    "        grouped_all_trials_df[\"n_trials\"] = grouped_all_trials_df[granger_col].apply(lambda x: np.sum(~np.isnan(x), axis=0))\n",
    "        grouped_all_trials_df[\"sem_granger\"] = grouped_all_trials_df.apply(lambda x: x[\"std_granger\"] / np.sqrt(x[\"n_trials\"]), axis=1)\n",
    "        for index, row in grouped_all_trials_df.iterrows():\n",
    "            try:\n",
    "                if trial_or_baseline == \"trial\":\n",
    "                    ax = sns.lineplot(x=all_frequencies, y=row[\"mean_granger\"], \\\n",
    "                label=\"{} {}\".format(row[GROUPINGS], trial_or_baseline), color=BASELINE_OUTCOME_TO_COLOR[row[GROUPINGS]],\n",
    "                linestyle=TRIAL_OR_BASELINE_TO_STYLE[trial_or_baseline], linewidth=3)\n",
    "                else:\n",
    "                    ax = sns.lineplot(x=all_frequencies, y=row[\"mean_granger\"], \\\n",
    "                color=BASELINE_OUTCOME_TO_COLOR[row[GROUPINGS]],\n",
    "                linestyle=TRIAL_OR_BASELINE_TO_STYLE[trial_or_baseline], linewidth=3)\n",
    "                \n",
    "\n",
    "    \n",
    "                plt.fill_between(all_frequencies, \\\n",
    "                row[\"mean_granger\"] - row[\"sem_granger\"], row[\"mean_granger\"] + row[\"sem_granger\"], alpha=0.1,\n",
    "                color=BASELINE_OUTCOME_TO_COLOR[row[GROUPINGS]])\n",
    "            except Exception as e: \n",
    "                print(e)\n",
    "\n",
    "\n",
    "    \n",
    "    # plt.savefig(\"./proc/granger/{}_{}hz_{}_granger_of_zscored_lfp.png\".format(low_freq, high_freq, pair_base_name))\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise ValueError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASIC_LFP_TRACES_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASIC_LFP_TRACES_DF[\"BLA_trial_lfp_trace\"]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for region_1, region_2 in trial_brain_region_pairs:\n",
    "    pair_base_name = \"{}_{}\".format(region_1.split(\"_\")[0], region_2.split(\"_\")[0])\n",
    "    print(pair_base_name)\n",
    "    try:\n",
    "        multitaper_col = \"{}_trial_multitaper\".format(pair_base_name)\n",
    "        BASIC_LFP_TRACES_DF[multitaper_col] = BASIC_LFP_TRACES_DF.apply(lambda x: Multitaper(time_series=np.array([x[region_1],x[region_2]]).T, sampling_frequency=RESAMPLE_RATE, time_halfbandwidth_product=5, time_window_duration=10), axis=1)\n",
    "    \n",
    "        connectivity_col = \"{}_trial_connectivity\".format(pair_base_name)\n",
    "        BASIC_LFP_TRACES_DF[connectivity_col] = BASIC_LFP_TRACES_DF[multitaper_col].apply(lambda x: Connectivity.from_multitaper(x))\n",
    "        \n",
    "        BASIC_LFP_TRACES_DF[\"{}_trial_frequencies\".format(pair_base_name)] = BASIC_LFP_TRACES_DF[connectivity_col].apply(lambda x: x.frequencies)\n",
    "    \n",
    "    except Exception as e: \n",
    "        print(e)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASIC_LFP_TRACES_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = BASIC_LFP_TRACES_DF[\"BLA_LH_trial_connectivity\"].iloc[0].pairwise_spectral_granger_prediction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = BASIC_LFP_TRACES_DF[\"BLA_LH_trial_connectivity\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example[:,:100][0][1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example[:,:100][0][4][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example[:,:100,1,0].mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example[:,:100,0,1].mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "example.pairwise_spectral_granger_prediction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_freq =30\n",
    "high_freq = 90\n",
    "for pair_base_name in all_pair_base_name:\n",
    "    print(pair_base_name)\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    plt.title(\"granger of Z-scored LFP in {}\".format(pair_base_name))\n",
    "    plt.xlabel(\"Frequency\")\n",
    "    plt.ylabel(\"granger of Z-scored LFP\")\n",
    "    plt.xlim(low_freq, high_freq)\n",
    "    plt.xticks(np.arange(low_freq, high_freq+1, 5))\n",
    "    plt.yticks(np.arange(0, 1, 0.1))\n",
    "    plt.ylim(0,1)\n",
    "    plt.grid()\n",
    "\n",
    "    for trial_or_baseline in TRIAL_AND_BASELINE:\n",
    "\n",
    "        granger_col = \"{}_{}_granger_magnitude\".format(pair_base_name, trial_or_baseline)\n",
    "        grouped_all_trials_df = channel_map_and_all_trials_df.groupby([GROUPINGS]).agg({granger_col: lambda x: np.vstack(x.tolist())}).reset_index()\n",
    "        # grouped_all_trials_df = grouped_all_trials_df[grouped_all_trials_df[\"trial_or_baseline\"] == \"trial\"]\n",
    "        grouped_all_trials_df[\"mean_granger\"] = grouped_all_trials_df[granger_col].apply(lambda x: np.nanmean(np.vstack(x), axis=0))\n",
    "        grouped_all_trials_df[\"std_granger\"] = grouped_all_trials_df[granger_col].apply(lambda x: np.nanstd(np.vstack(x), axis=0))\n",
    "        grouped_all_trials_df[\"n_trials\"] = grouped_all_trials_df[granger_col].apply(lambda x: np.sum(~np.isnan(x), axis=0))\n",
    "        grouped_all_trials_df[\"sem_granger\"] = grouped_all_trials_df.apply(lambda x: x[\"std_granger\"] / np.sqrt(x[\"n_trials\"]), axis=1)\n",
    "        for index, row in grouped_all_trials_df.iterrows():\n",
    "            try:\n",
    "                if trial_or_baseline == \"trial\":\n",
    "                    ax = sns.lineplot(x=all_frequencies, y=row[\"mean_granger\"], \\\n",
    "                label=\"{} {}\".format(row[GROUPINGS], trial_or_baseline), color=BASELINE_OUTCOME_TO_COLOR[row[GROUPINGS]],\n",
    "                linestyle=TRIAL_OR_BASELINE_TO_STYLE[trial_or_baseline], linewidth=3)\n",
    "                else:\n",
    "                    ax = sns.lineplot(x=all_frequencies, y=row[\"mean_granger\"], \\\n",
    "                color=BASELINE_OUTCOME_TO_COLOR[row[GROUPINGS]],\n",
    "                linestyle=TRIAL_OR_BASELINE_TO_STYLE[trial_or_baseline], linewidth=3)\n",
    "                \n",
    "\n",
    "    \n",
    "                plt.fill_between(all_frequencies, \\\n",
    "                row[\"mean_granger\"] - row[\"sem_granger\"], row[\"mean_granger\"] + row[\"sem_granger\"], alpha=0.1,\n",
    "                color=BASELINE_OUTCOME_TO_COLOR[row[GROUPINGS]])\n",
    "            except Exception as e: \n",
    "                print(e)\n",
    "\n",
    "\n",
    "    \n",
    "    # plt.savefig(\"./proc/granger/{}_{}hz_{}_granger_of_zscored_lfp.png\".format(low_freq, high_freq, pair_base_name))\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise ValueError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "9b36cdf08567463082b005cb0dec684b",
    "deepnote_cell_type": "text-cell-p",
    "formattedRanges": []
   },
   "source": [
    "Describe what is done to the data here and how inputs are manipulated to generate outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "89aaba237c644628b1b37604b75e7cb1",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "# As much code and as many cells as required\n",
    "# includes EDA and playing with data\n",
    "# GO HAM!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating average power per band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in BASIC_LFP_TRACES_DF.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_averaged_power_columns = [col for col in BASIC_LFP_TRACES_DF.columns if \"trial_lfp_trace\" in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_averaged_power_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for region_1 in chunk_averaged_power_columns:\n",
    "    for region_2 in chunk_averaged_power_columns:\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASIC_LFP_TRACES_DF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([BASIC_LFP_TRACES_DF.iloc[0][region_1], BASIC_LFP_TRACES_DF.iloc[0][region_2]]).T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for region_1, region_2 in permutations(chunk_averaged_power_columns, 2):\n",
    "    pair_base_name = \"{}_{}\".format(region_1.split(\"_\")[0], region_2.split(\"_\")[0],)\n",
    "    print(pair_base_name)\n",
    "    try:\n",
    "        \n",
    "        # granger_value = grangercausalitytests(BASIC_LFP_TRACES_DF[[region_1, region_2]], maxlag=[3])\n",
    "        BASIC_LFP_TRACES_DF[\"{}_granger\".format(pair_base_name)] = BASIC_LFP_TRACES_DF.apply(lambda row: get_single_granger_causality(arr1=row[region_1], arr2=row[region_2]), axis=1)\n",
    "        print()\n",
    "    except Exception as e: \n",
    "        print(e)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASIC_LFP_TRACES_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for region_1, region_2 in permutations(chunk_averaged_power_columns, 2):\n",
    "    pair_base_name = \"{}_{}\".format(region_1.strip(\"trace\").strip(\"_\"), region_2.strip(\"trace\").strip(\"_\"))\n",
    "    print(pair_base_name)\n",
    "    try:\n",
    "        \n",
    "        # granger_value = grangercausalitytests(BASIC_LFP_TRACES_DF[[region_1, region_2]], maxlag=[3])\n",
    "        BASIC_LFP_TRACES_DF[\"{}_granger\".format(pair_base_name)] = BASIC_LFP_TRACES_DF.apply(lambda row: grangercausalitytests(np.array([row[region_1], row[region_2]]).T, maxlag=1), axis=1)\n",
    "        print()\n",
    "    except Exception as e: \n",
    "        print(e)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Power correlation between brain regions calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Combining the trial/baseline and outcome label for coloring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASIC_LFP_TRACES_DF[\"outcome_and_trial_or_baseline\"] = BASIC_LFP_TRACES_DF.apply(lambda x: \"_\".join([x[\"trial_outcome\"], x[\"trial_or_baseline\"]]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASIC_LFP_TRACES_DF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_columns = [col for col in BASIC_LFP_TRACES_DF.columns if \"trace\" in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain_region_pairs = generate_pairs(trace_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain_region_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grangercausalitytests(df[['column1', 'column2']], maxlag=[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASIC_LFP_TRACES_DF[region_1].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for region_1, region_2 in brain_region_pairs:\n",
    "    pair_base_name = \"{}_{}\".format(region_1.strip(\"trace\").strip(\"_\"), region_2.strip(\"trace\").strip(\"_\"))\n",
    "    print(pair_base_name)\n",
    "    try:\n",
    "        \n",
    "        # granger_value = grangercausalitytests(BASIC_LFP_TRACES_DF[[region_1, region_2]], maxlag=[3])\n",
    "        BASIC_LFP_TRACES_DF[\"{}_granger\".format(pair_base_name)] = BASIC_LFP_TRACES_DF.apply(lambda row: grangercausalitytests(np.array([row[region_1], row[region_2]]).T, maxlag=[3]), axis=1)\n",
    "        print()\n",
    "    except Exception as e: \n",
    "        print(e)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASIC_LFP_TRACES_DF[\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASIC_LFP_TRACES_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "granger_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Filtering out the outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "band_to_power_correlation = defaultdict(dict)\n",
    "for band in ALL_BANDS:\n",
    "    # Getting all the pairs of brain regions\n",
    "    band_averaged_columns = [col for col in BASIC_LFP_TRACES_DF.columns if \"averaged_{}\".format(band) in col]\n",
    "    band_to_power_correlation[band][\"brain_region_pairs\"] = generate_pairs(band_averaged_columns)\n",
    "    print(band_to_power_correlation[band][\"brain_region_pairs\"])\n",
    "\n",
    "    # Removing rows that are outliers\n",
    "    filtered_df = BASIC_LFP_TRACES_DF.copy()\n",
    "    \n",
    "    for col in band_averaged_columns:\n",
    "        # filtered_df = filtered_df[filtered_df[col] <= 3]\n",
    "        # Assuming data is a 1D numpy array\n",
    "        Q1 = np.percentile(filtered_df[col], 25)\n",
    "        Q3 = np.percentile(filtered_df[col], 75)\n",
    "        IQR = Q3 - Q1\n",
    "        band_to_power_correlation[band][\"outlier_removed_df\"] = filtered_df[(filtered_df[col] >= Q1 - 1.5 * IQR) & (filtered_df[col] <= Q3 + 1.5 * IQR)]\n",
    "\n",
    "\n",
    "    \n",
    "    # Getting the mean and standard deviation\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "band_to_power_correlation = defaultdict(dict)\n",
    "for band in ALL_BANDS:\n",
    "    # Getting all the pairs of brain regions\n",
    "    band_averaged_columns = [col for col in BASIC_LFP_TRACES_DF.columns if \"averaged_{}\".format(band) in col]\n",
    "    band_to_power_correlation[band][\"brain_region_pairs\"] = generate_pairs(band_averaged_columns)\n",
    "    print(band_to_power_correlation[band][\"brain_region_pairs\"])\n",
    "\n",
    "    # Removing rows that are outliers\n",
    "    filtered_df = BASIC_LFP_TRACES_DF.copy()\n",
    "    \n",
    "    for col in band_averaged_columns:\n",
    "        # filtered_df = filtered_df[filtered_df[col] <= 3]\n",
    "        # Assuming data is a 1D numpy array\n",
    "        Q1 = np.percentile(filtered_df[col], 25)\n",
    "        Q3 = np.percentile(filtered_df[col], 75)\n",
    "        IQR = Q3 - Q1\n",
    "        filtered_df = filtered_df[(filtered_df[col] >= Q1 - 1.5 * IQR) & (filtered_df[col] <= Q3 + 1.5 * IQR)]\n",
    "    band_to_power_correlation[band][\"outlier_removed_df\"] = filtered_df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "band_to_power_correlation = defaultdict(dict)\n",
    "for band in ALL_BANDS:\n",
    "    # Getting all the pairs of brain regions\n",
    "    band_averaged_columns = [col for col in BASIC_LFP_TRACES_DF.columns if \"averaged_{}\".format(band) in col]\n",
    "    band_to_power_correlation[band][\"brain_region_pairs\"] = generate_pairs(band_averaged_columns)\n",
    "    print(band_to_power_correlation[band][\"brain_region_pairs\"])\n",
    "    \n",
    "    # code to filter based on std\n",
    "    filtered_df = BASIC_LFP_TRACES_DF.copy()\n",
    "    for col in band_averaged_columns:\n",
    "        # Assuming data is a 1D numpy array\n",
    "        threshold = 1\n",
    "        mean = np.median(filtered_df[col])\n",
    "        std = np.std(filtered_df[col])\n",
    "        filtered_df = filtered_df[np.abs(filtered_df[col] - mean) < threshold * std]\n",
    "    band_to_power_correlation[band][\"outlier_removed_df\"] = filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASIC_LFP_TRACES_DF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "band_to_power_correlation[band][\"outlier_removed_df\"].shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# code to filter based on std\n",
    "filtered_df = BASIC_LFP_TRACES_DF.copy()\n",
    "for col in averaged_col:\n",
    "    # Assuming data is a 1D numpy array\n",
    "    threshold = 3.5\n",
    "    mean = np.mean(filtered_df[col])\n",
    "    std = np.std(filtered_df[col])\n",
    "    filtered_df = filtered_df[np.abs(filtered_df[col] - mean) < threshold * std]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Plotting all of the conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for band in ALL_BANDS:\n",
    "    for region_1, region_2 in band_to_power_correlation[band][\"brain_region_pairs\"]:\n",
    "        region_1_basename = region_1.split(\"_\")[0]\n",
    "        region_2_basename = region_2.split(\"_\")[0]\n",
    "        x = band_to_power_correlation[band][\"outlier_removed_df\"][region_1]\n",
    "        y = band_to_power_correlation[band][\"outlier_removed_df\"][region_2]\n",
    "        \n",
    "        # Perform linear regression to get the slope, intercept and r-value (correlation coefficient)\n",
    "        slope, intercept, r_value, p_value, std_err = linregress(x, y)\n",
    "        \n",
    "        # Create a line of best fit using the slope and intercept\n",
    "        line = slope * x + intercept\n",
    "        \n",
    "        # Create scatter plot\n",
    "        sns.scatterplot(x=x, y=y, data=band_to_power_correlation[band][\"outlier_removed_df\"], hue='outcome_and_trial_or_baseline', palette=BASELINE_OUTCOME_TO_COLOR)\n",
    "        \n",
    "        # Plot line of best fit\n",
    "        plt.plot(x, line, color='red')\n",
    "        \n",
    "        # Add R² value to the plot\n",
    "        plt.text(0.1, 0.9, f'R = {r_value:.2f}', transform=plt.gca().transAxes)\n",
    "        \n",
    "        # Add labels and legend\n",
    "        plt.title(\"Power correlation of Z-scored {} band LFP: {} and {}\".format(band, region_2_basename, region_1_basename))\n",
    "        plt.xlabel('{} {} power of Z-scored LFP'.format(band, region_1_basename))\n",
    "        plt.ylabel('{} {} power of Z-scored LFP'.format(band, region_2_basename))\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"./proc/power_correlation/zscored/{}/all_condition_{}_{}_power_correlation_of_zscored_{}_lfp.png\".format(band, region_1_basename, region_2_basename, band))\n",
    "        # Display the plot\n",
    "        plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise ValueError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASIC_LFP_TRACES_DF = filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASIC_LFP_TRACES_DF[\"trial_outcome\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASIC_LFP_TRACES_DF[\"trial_or_baseline\"]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for band in ALL_BANDS:\n",
    "    for region_1, region_2 in band_to_power_correlation[band][\"brain_region_pairs\"]:\n",
    "        region_1_basename = region_1.split(\"_\")[0]\n",
    "        region_2_basename = region_2.split(\"_\")[0]\n",
    "        x = band_to_power_correlation[band][\"outlier_removed_df\"][region_1]\n",
    "        y = band_to_power_correlation[band][\"outlier_removed_df\"][region_2]\n",
    "        \n",
    "        # Perform linear regression to get the slope, intercept and r-value (correlation coefficient)\n",
    "        slope, intercept, r_value, p_value, std_err = linregress(x, y)\n",
    "        \n",
    "        # Create a line of best fit using the slope and intercept\n",
    "        line = slope * x + intercept\n",
    "        \n",
    "        # Create scatter plot\n",
    "        sns.scatterplot(x=x, y=y, data=band_to_power_correlation[band][\"outlier_removed_df\"], hue='outcome_and_trial_or_baseline', palette=BASELINE_OUTCOME_TO_COLOR)\n",
    "        \n",
    "        # Plot line of best fit\n",
    "        plt.plot(x, line, color='red')\n",
    "        \n",
    "        # Add R² value to the plot\n",
    "        plt.text(0.1, 0.9, f'R = {r_value:.2f}', transform=plt.gca().transAxes)\n",
    "        \n",
    "        # Add labels and legend\n",
    "        plt.title(\"Power correlation of Z-scored {} band LFP: {} and {}\".format(band, region_2_basename, region_1_basename))\n",
    "        plt.xlabel('{} {} power of Z-scored LFP'.format(band, region_1_basename))\n",
    "        plt.ylabel('{} {} power of Z-scored LFP'.format(band, region_2_basename))\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        # plt.savefig(\"./proc/power_correlation/zscored/all_condition_{}_{}_power_correlation_of_zscored_theta_lfp.png\".format(region_1_basename, region_2_basename))\n",
    "        # Display the plot\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for band in ALL_BANDS:\n",
    "    band_df = band_to_power_correlation[band][\"outlier_removed_df\"]\n",
    "    band_to_power_correlation[band][\"region_pair_to_outcome_to_r2\"] = defaultdict(nested_dict)\n",
    "    for outcome in band_df[\"trial_outcome\"].unique():\n",
    "        outcome_df = band_df[band_df[\"trial_outcome\"] == outcome]\n",
    "        for region_1, region_2 in brain_region_pairs:\n",
    "            region_1_basename = region_1.split(\"_\")[0]\n",
    "            region_2_basename = region_2.split(\"_\")[0]\n",
    "            \n",
    "            x = outcome_df[region_1]\n",
    "            y = outcome_df[region_2]\n",
    "            \n",
    "            # Perform linear regression to get the slope, intercept and r-value (correlation coefficient)\n",
    "            slope, intercept, r_value, p_value, std_err = linregress(x, y)\n",
    "            # Square the r value to get the r squared value\n",
    "            r2_value = r_value**2\n",
    "            band_to_power_correlation[band][\"region_pair_to_outcome_to_r2\"][\"{}_{}\".format(region_1.split(\"_\")[0], region_2.split(\"_\")[0])][outcome][\"r\"] = r_value\n",
    "            band_to_power_correlation[band][\"region_pair_to_outcome_to_r2\"][\"{}_{}\".format(region_1.split(\"_\")[0], region_2.split(\"_\")[0])][outcome][\"std\"] = std_err\n",
    "            \n",
    "            # Create a line of best fit using the slope and intercept\n",
    "            line = slope * x + intercept\n",
    "            \n",
    "            # Create scatter plot\n",
    "            sns.scatterplot(x=x, y=y, data=outcome_df, hue='outcome_and_trial_or_baseline', palette=BASELINE_OUTCOME_TO_COLOR, style='outcome_and_trial_or_baseline', markers=['^', 'o'])\n",
    "            \n",
    "            # Plot line of best fit\n",
    "            plt.plot(x, line, color='red')\n",
    "            \n",
    "            # Add R² value to the plot\n",
    "            plt.text(0.1, 0.9, f'R = {r_value:.2f}', transform=plt.gca().transAxes)\n",
    "            \n",
    "            # Add labels and legend\n",
    "            plt.title(\"Power Correlation of Z-scored {} LFP: {} and {}\".format(band, region_2_basename, region_1_basename))\n",
    "            plt.xlabel('{} {} Power of Z-scored LFP'.format(region_1_basename, band))\n",
    "            plt.ylabel('{} {} Power of Z-scored LFP'.format(region_2_basename, band))\n",
    "            plt.legend(loc=\"lower right\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(\"./proc/power_correlation/zscored/{}/{}_{}_{}_power_correlation_of_zscored_{}_lfp.png\".format(band, outcome, region_1_basename, region_2_basename, band))\n",
    "            # Display the plot\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for band in ALL_BANDS:\n",
    "    # Convert the nested dictionary to a DataFrame\n",
    "    data = []\n",
    "    for group_name, group_data in band_to_power_correlation[band]['region_pair_to_outcome_to_r2'].items():\n",
    "        for bar_name, bar_dict in group_data.items():\n",
    "            data.append({\"Group\": group_name, \"Bar\": bar_name, \"r\": bar_dict[\"r\"], \"std\": bar_dict[\"std\"]})\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Create the bar plot using seaborn\n",
    "    # sns.catplot(\n",
    "    #     data=df, \n",
    "    #     x='Group', \n",
    "    #     y='r2', \n",
    "    #     hue='Bar', \n",
    "    #     kind='bar', \n",
    "    #     height=4, \n",
    "    #     aspect=2,\n",
    "    #     legend=False,\n",
    "    #     # yerr=df['std'].values,  # This line adds the SEM bars\n",
    "    #     # capsize=0.1  # This line adds caps on the error bars\n",
    "    # )\n",
    "    \n",
    "    # Create barplot\n",
    "    ax = sns.barplot(x='Group', y='r', hue='Bar', data=df, ci=None)\n",
    "    \n",
    "    # Adding error bars\n",
    "    groups = df['Group'].unique()\n",
    "    bars_per_group = df['Bar'].nunique()\n",
    "    bar_width = 0.8 / bars_per_group\n",
    "    x_positions = []\n",
    "    \n",
    "    for i, group in enumerate(groups):\n",
    "        num_bars = df[df['Group'] == group].shape[0]\n",
    "        group_positions = np.linspace(i - bar_width*(num_bars-1)/2, i + bar_width*(num_bars-1)/2, num_bars)\n",
    "        x_positions.extend(group_positions)\n",
    "    \n",
    "    for i, (r2, sem) in enumerate(zip(df['r'], df['std'])):\n",
    "        plt.errorbar(x_positions[i], r2, yerr=sem, fmt='none', color='black', capsize=5)\n",
    "    \n",
    "    \n",
    "    plt.xticks(rotation=90)\n",
    "    plt.xlabel(\"Brain region pairs\")\n",
    "    plt.ylabel(\"Power correlation r\")\n",
    "    plt.legend(title=\"Trial Conditions\")\n",
    "    plt.title(\"{} Power correlations\".format(band))\n",
    "    plt.tight_layout()\n",
    "    plt.grid()\n",
    "    \n",
    "    plt.savefig(\"./proc/power_correlation/zscored/all_zscored_{}_lfp_power_correlation.png\".format(band))\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Convert the nested dictionary to a DataFrame\n",
    "data = []\n",
    "for group_name, group_data in region_pair_to_outcome_to_r2.items():\n",
    "    for bar_name, bar_dict in group_data.items():\n",
    "        data.append({\"Group\": group_name, \"Bar\": bar_name, \"r\": bar_dict[\"r\"], \"std\": bar_dict[\"std\"]})\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Create the bar plot using seaborn\n",
    "# sns.catplot(\n",
    "#     data=df, \n",
    "#     x='Group', \n",
    "#     y='r2', \n",
    "#     hue='Bar', \n",
    "#     kind='bar', \n",
    "#     height=4, \n",
    "#     aspect=2,\n",
    "#     legend=False,\n",
    "#     # yerr=df['std'].values,  # This line adds the SEM bars\n",
    "#     # capsize=0.1  # This line adds caps on the error bars\n",
    "# )\n",
    "\n",
    "# Create barplot\n",
    "ax = sns.barplot(x='Group', y='r', hue='Bar', data=df, ci=None)\n",
    "\n",
    "# Adding error bars\n",
    "groups = df['Group'].unique()\n",
    "bars_per_group = df['Bar'].nunique()\n",
    "bar_width = 0.8 / bars_per_group\n",
    "x_positions = []\n",
    "\n",
    "for i, group in enumerate(groups):\n",
    "    num_bars = df[df['Group'] == group].shape[0]\n",
    "    group_positions = np.linspace(i - bar_width*(num_bars-1)/2, i + bar_width*(num_bars-1)/2, num_bars)\n",
    "    x_positions.extend(group_positions)\n",
    "\n",
    "for i, (r2, sem) in enumerate(zip(df['r'], df['std'])):\n",
    "    plt.errorbar(x_positions[i], r2, yerr=sem, fmt='none', color='black', capsize=5)\n",
    "\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel(\"Brain region pairs\")\n",
    "plt.ylabel(\"Power correlation r\")\n",
    "plt.legend(title=\"Trial Conditions\")\n",
    "plt.title(\"Power correlations\")\n",
    "plt.tight_layout()\n",
    "plt.grid()\n",
    "\n",
    "plt.savefig(\"./proc/power_correlation/zscored/all_zscored_lfp_power_correlation.png\")\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import statannot\n",
    "statannot.add_stat_annotation(\n",
    "    ax,\n",
    "    data=df,\n",
    "    x=x,\n",
    "    y=y,\n",
    "    hue=hue,\n",
    "    box_pairs=[\n",
    "        ((\"Biscoe\", \"Male\"), (\"Torgersen\", \"Female\")),\n",
    "        ((\"Dream\", \"Male\"), (\"Dream\", \"Female\")),\n",
    "    ],\n",
    "    test=\"t-test_ind\",\n",
    "    text_format=\"star\",\n",
    "    loc=\"outside\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "groups = ['Group1', 'Group2', 'Group3']\n",
    "values = [1.2, 2.3, 1.8]\n",
    "\n",
    "# Dictionary holding the results of your statistical comparisons\n",
    "# Keys are tuples indicating the groups being compared; values are the corresponding p-values\n",
    "stats_dict = {('Group1', 'Group2'): 0.04, ('Group1', 'Group3'): 0.01, ('Group2', 'Group3'): 0.01}\n",
    "\n",
    "# Thresholds for significance levels\n",
    "alpha = 0.05  # usually 0.05\n",
    "alpha_strong = 0.01  # example value\n",
    "\n",
    "# Create bar plot\n",
    "fig, ax = plt.subplots()\n",
    "bars = plt.bar(groups, values, color=['blue', 'green', 'red'])\n",
    "\n",
    "# Get the y-axis limits\n",
    "bottom, top = ax.get_ylim()\n",
    "y_range = top - bottom\n",
    "\n",
    "# Retrieve x-coordinates of the bars\n",
    "x_coords = [bar.get_x() + bar.get_width() / 2.0 for bar in bars]  # get_x() retrieves the left coordinate, we adjust by half the width to get the center\n",
    "group_to_x_coord = {group: x_coord for group, x_coord in zip(groups, x_coords)}\n",
    "\n",
    "\n",
    "# Significance bars\n",
    "for i, (key, value) in enumerate(stats_dict.items()):\n",
    "    # Significance level\n",
    "    p = value\n",
    "    if p < 0.001:\n",
    "        sig_symbol = '***'\n",
    "    elif p < 0.01:\n",
    "        sig_symbol = '**'\n",
    "    elif p < 0.05:\n",
    "        sig_symbol = '*'\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    # Columns corresponding to the datasets of interest\n",
    "    x1 = key[0]\n",
    "    x2 = key[1]\n",
    "    # What level is this bar among the bars above the plot?\n",
    "    level = len(stats_dict) - i\n",
    "    # Plot the bar\n",
    "    bar_height = (y_range * 0.15 * level) + top\n",
    "    bar_tips = bar_height - (y_range * 0.02)\n",
    "    plt.plot(\n",
    "        [x1, x1, x2, x2],\n",
    "        [bar_tips, bar_height, bar_height, bar_tips], lw=1, c='k'\n",
    "    )\n",
    "    \n",
    "    text_height = bar_height + (y_range * 0.01)\n",
    "    plt.text((group_to_x_coord[x1] + group_to_x_coord[x2]) * 0.5, text_height, sig_symbol, ha='center', va='bottom', c='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "groups = ['Group1', 'Group2', 'Group3']\n",
    "values = [1.2, 2.3, 1.8]\n",
    "\n",
    "# Dictionary holding the results of your statistical comparisons\n",
    "# Keys are tuples indicating the groups being compared; values are the corresponding p-values\n",
    "stats_dict = {('Group1', 'Group2'): 0.04, ('Group1', 'Group3'): 0.01, ('Group2', 'Group3'): 0.01}\n",
    "\n",
    "# Thresholds for significance levels\n",
    "alpha = 0.05  # usually 0.05\n",
    "alpha_strong = 0.01  # example value\n",
    "\n",
    "# Create bar plot\n",
    "fig, ax = plt.subplots()\n",
    "bars = plt.bar(groups, values, color=['blue', 'green', 'red'])\n",
    "\n",
    "# Function to draw significance bars\n",
    "def draw_significance_bar(ax, x1, x2, y, height, text, offset):\n",
    "    ax.plot([x1, x1, x2, x2], [y + offset, y + height + offset, y + height + offset, y + offset], lw=1.5, c='black')\n",
    "    ax.text((x1 + x2) * .5, y + height + offset, text, ha='center', va='bottom', color='black')\n",
    "\n",
    "# Height of the small vertical ticks and offset for multiple comparisons\n",
    "tick_height = 0.1\n",
    "offset_step = 0.1  # How much to offset each additional line\n",
    "\n",
    "# Track the current offset\n",
    "current_offsets = {group: 0 for group in groups}\n",
    "\n",
    "# Check each comparison\n",
    "for comparison, p_value in stats_dict.items():\n",
    "    # Determine the index of the groups\n",
    "    index_1 = groups.index(comparison[0])\n",
    "    index_2 = groups.index(comparison[1])\n",
    "\n",
    "    # Set the positions of bars on X axis\n",
    "    x1, x2 = bars[index_1].xy[0] + 0.4, bars[index_2].xy[0] + 0.4  # we use 0.4 to center the line on the bar\n",
    "    max_height = max([bar.get_height() for bar in bars])\n",
    "    y = values[index_1] if values[index_1] > values[index_2] else values[index_2]  # get the maximum value among the two groups\n",
    "    \n",
    "    # Choose the symbol to display depending on the p-value\n",
    "    if p_value < alpha_strong:\n",
    "        text = '**'  # or '***' for an even smaller p-value\n",
    "    elif p_value < alpha:\n",
    "        text = '*'\n",
    "    else:\n",
    "        continue  # Don't draw a bar if not significant\n",
    "\n",
    "    # Determine the offset for this line\n",
    "    offset = max(current_offsets[comparison[0]], current_offsets[comparison[1]])\n",
    "    \n",
    "    # Draw the significance bar\n",
    "    draw_significance_bar(ax, x1, x2, y, tick_height, text, offset)\n",
    "\n",
    "    # Update the current offset for these groups\n",
    "    current_offsets[comparison[0]] = current_offsets[comparison[1]] = offset + offset_step\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "cf8fe3695d074ee7887fdf6459cbf5ce",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
